{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape, Conv2D, Dense, Flatten, BatchNormalization, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Box(0, 255, (96, 96, 3), uint8)\n(96, 96, 3)\nBox(-1.0, 1.0, (3,), float32)\n(3,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v0')\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space)\n",
    "print(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CarRacingDiscrit:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CarRacing-v0')\n",
    "        self.action_space = 10*10*10\n",
    "        self.observation_space = 96*96*3\n",
    "\n",
    "    def step(self, action):\n",
    "        v1 = int(     action        ) % 10\n",
    "        v2 = int( int(action) / 10  ) % 10\n",
    "        v3 = int( int(action) / 100 ) % 10\n",
    "        v1 = ( v1 - 5 ) / 5\n",
    "        v2 = ( v2     ) / 10\n",
    "        v3 = ( v3     ) / 10\n",
    "        state, reward, done, info = self.env.step([v1, v2, v3])\n",
    "        return state, reward, done, info\n",
    " \n",
    "    def seed(self, s):\n",
    "        return env.seed(s)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "27648\n1000\n"
     ]
    }
   ],
   "source": [
    "# Get the environment and extract the number of actions.\n",
    "env = CarRacingDiscrit()\n",
    "#env.seed(123)\n",
    "nb_actions = 10*10*10\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nreshape (Reshape)            (None, 96, 96, 3)         0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 96, 96, 3)         12        \n_________________________________________________________________\nconv2d (Conv2D)              (None, 94, 94, 32)        896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 47, 47, 32)        0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 47, 47, 32)        128       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 45, 45, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 22, 22, 64)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 30976)             0         \n_________________________________________________________________\ndense (Dense)                (None, 1000)              30977000  \n_________________________________________________________________\ndropout (Dropout)            (None, 1000)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1000)              1001000   \n=================================================================\nTotal params: 31,997,532\nTrainable params: 31,997,462\nNon-trainable params: 70\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Reshape((96, 96, 3), input_shape=(1, 96, 96, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8192, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in tensorflow.keras optimizer and\n",
    "# even the metrics!\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=5000, window_length=1)\n",
    "agent = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=nb_actions,\n",
    "                 nb_steps_warmup=500, target_model_update=1e-2)\n",
    "agent.compile(Adam(lr=1e-3), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training for 50000 steps ...\n",
      "Track generation: 1186..1486 -> 300-tiles track\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      " 1000/10000 [==>...........................] - ETA: 51:59 - reward: -0.0799Track generation: 1277..1601 -> 324-tiles track\n",
      " 2000/10000 [=====>........................] - ETA: 1:05:38 - reward: -0.0760Track generation: 1192..1495 -> 303-tiles track\n",
      "done, took 1001.120 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3c0fac4ee0>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "agent.fit(env, nb_steps=50000, visualize=False, verbose=1, nb_max_episode_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('ddpg_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing for 5 episodes ...\n",
      "Track generation: 1140..1429 -> 289-tiles track\n",
      "Episode 1: reward: 7.461, steps: 99\n",
      "Track generation: 1194..1497 -> 303-tiles track\n",
      "Episode 2: reward: 6.656, steps: 99\n",
      "Track generation: 954..1202 -> 248-tiles track\n",
      "Episode 3: reward: 10.343, steps: 99\n",
      "Track generation: 1093..1378 -> 285-tiles track\n",
      "Episode 4: reward: 7.706, steps: 99\n",
      "Track generation: 1077..1350 -> 273-tiles track\n",
      "Episode 5: reward: 8.482, steps: 99\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ba805e820>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env, nb_episodes=5, nb_max_episode_steps=2000, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}